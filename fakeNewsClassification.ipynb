{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re,string,nltk\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\nfrom nltk.tokenize import word_tokenize \nimport numpy as np\nimport pandas as pd\nimport spacy\nnlp = spacy.load('en_core_web_sm')\nimport string\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer  # https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom collections import Counter\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import f1_score\nimport math\nfrom sklearn.naive_bayes import MultinomialNB","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-01-13T11:50:52.050228Z","iopub.execute_input":"2023-01-13T11:50:52.050630Z","iopub.status.idle":"2023-01-13T11:51:05.666311Z","shell.execute_reply.started":"2023-01-13T11:50:52.050491Z","shell.execute_reply":"2023-01-13T11:51:05.665139Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/d/ritikabhardwaj2801/mediaeval2015/mediaeval-2015-trainingset.txt', sep = '\\t')\n#data.head()\ndata.columns = [\"tweetId\", \"tweetText\", \"userId\", \"imageId(s)\",\"username\",\"timestamp\",\"label\"]\ndata.dropna(inplace=True)\ndata=data.drop_duplicates(subset=['tweetText'])\ndef labels_to_numbers(label):\n    if label[0] == 'r':\n        return 0\n    else:\n        return 1\ndata['label'] = data['label'].apply(labels_to_numbers)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T12:51:52.817785Z","iopub.execute_input":"2023-01-13T12:51:52.818073Z","iopub.status.idle":"2023-01-13T12:51:52.903223Z","shell.execute_reply.started":"2023-01-13T12:51:52.818042Z","shell.execute_reply":"2023-01-13T12:51:52.902604Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"def get_number_of_words(text):\n    x=text.split(\" \")\n    count=len(x)\n    return count","metadata":{"execution":{"iopub.status.busy":"2023-01-13T13:03:39.549691Z","iopub.execute_input":"2023-01-13T13:03:39.549997Z","iopub.status.idle":"2023-01-13T13:03:39.555219Z","shell.execute_reply.started":"2023-01-13T13:03:39.549967Z","shell.execute_reply":"2023-01-13T13:03:39.554060Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"data['newFeature'] = data['tweetText'].apply(get_number_of_words)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T13:03:41.227723Z","iopub.execute_input":"2023-01-13T13:03:41.228000Z","iopub.status.idle":"2023-01-13T13:03:41.250454Z","shell.execute_reply.started":"2023-01-13T13:03:41.227970Z","shell.execute_reply":"2023-01-13T13:03:41.249602Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"def process_text(text): #should probably get rid of all the remaining hashtags ..or not \n    text = text.translate(str.maketrans('', '', string.punctuation)) #getting rid of the punctuation\n    text_tokens = nlp(text)\n    text_tokens = [token.lemma_ for token in text_tokens if not token.is_stop] #getting rid of hashtags and tags , and token != \"\" and token[0] not in ['@', '#'])\n    return \" \".join(text_tokens)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T13:03:43.120958Z","iopub.execute_input":"2023-01-13T13:03:43.121297Z","iopub.status.idle":"2023-01-13T13:03:43.127071Z","shell.execute_reply.started":"2023-01-13T13:03:43.121256Z","shell.execute_reply":"2023-01-13T13:03:43.126038Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"def preprocess_text(text):\n    text = text.lower()\n    new_text = []\n    for t in text.split(\" \"):\n        t = '' if (t.startswith('http')) else t #t = '' if (t.startswith('@') or t.startswith('#') or  t.startswith('http')) else t\n        if t != '':\n            new_text.append(t)\n    return \" \".join(new_text)\n\ndata['tweetText'] = data['tweetText'].apply(preprocess_text)\ndata=data.dropna()","metadata":{"execution":{"iopub.status.busy":"2023-01-13T13:03:44.647476Z","iopub.execute_input":"2023-01-13T13:03:44.647956Z","iopub.status.idle":"2023-01-13T13:03:44.724471Z","shell.execute_reply.started":"2023-01-13T13:03:44.647920Z","shell.execute_reply":"2023-01-13T13:03:44.723791Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"data['tweetTextTokens'] = data['tweetText'].apply(process_text)\ndata=data.dropna()\ndata = data.drop(columns=['timestamp', 'username', 'userId', 'tweetText'])","metadata":{"execution":{"iopub.status.busy":"2023-01-13T13:03:48.065404Z","iopub.execute_input":"2023-01-13T13:03:48.065842Z","iopub.status.idle":"2023-01-13T13:05:09.545279Z","shell.execute_reply.started":"2023-01-13T13:03:48.065798Z","shell.execute_reply":"2023-01-13T13:05:09.544308Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nstopwords.words('english')\n\ndef stopword_removal(my_txt):\n    filtered_list = []\n    stop_words = nltk.corpus.stopwords.words('english')\n# Tokenize the sentence\n    words = word_tokenize(my_txt)\n    for w in words:\n        if w.lower() not in stop_words:\n            filtered_list.append(w)\n    filtered_list\n    my_clean_txt = \" \".join(filtered_list)\n    return my_clean_txt\n\ndata['tweetTextTokens'] = data['tweetTextTokens'].apply(stopword_removal)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-13T11:57:24.285032Z","iopub.execute_input":"2023-01-13T11:57:24.285463Z","iopub.status.idle":"2023-01-13T11:57:28.213881Z","shell.execute_reply.started":"2023-01-13T11:57:24.285430Z","shell.execute_reply":"2023-01-13T11:57:28.212989Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"              tweetId      imageId(s)  label  newFeature  \\\n0  263046056240115712  sandyA_fake_46      1          22   \n1  262995061304852481  sandyA_fake_09      1          21   \n2  262979898002534400  sandyA_fake_09      1          19   \n3  262996108400271360  sandyA_fake_29      1           5   \n4  263018881839411200  sandyA_fake_15      1          12   \n\n                                     tweetTextTokens  \n0  ¿ se acuerdan de la película `` el día después...  \n1  milenagimon miren sandy en ny tremenda imagen ...  \n2  buena la foto del huracán sandy recuerda la pe...  \n3                            scary shit hurricane ny  \n4  fave place world nyc hurricane sandy statueofl...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweetId</th>\n      <th>imageId(s)</th>\n      <th>label</th>\n      <th>newFeature</th>\n      <th>tweetTextTokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>263046056240115712</td>\n      <td>sandyA_fake_46</td>\n      <td>1</td>\n      <td>22</td>\n      <td>¿ se acuerdan de la película `` el día después...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>262995061304852481</td>\n      <td>sandyA_fake_09</td>\n      <td>1</td>\n      <td>21</td>\n      <td>milenagimon miren sandy en ny tremenda imagen ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>262979898002534400</td>\n      <td>sandyA_fake_09</td>\n      <td>1</td>\n      <td>19</td>\n      <td>buena la foto del huracán sandy recuerda la pe...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>262996108400271360</td>\n      <td>sandyA_fake_29</td>\n      <td>1</td>\n      <td>5</td>\n      <td>scary shit hurricane ny</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>263018881839411200</td>\n      <td>sandyA_fake_15</td>\n      <td>1</td>\n      <td>12</td>\n      <td>fave place world nyc hurricane sandy statueofl...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Putting the test data in an usable format","metadata":{}},{"cell_type":"code","source":"test_data = pd.read_csv('/kaggle/input/d/ritikabhardwaj2801/mediaeval2015/mediaeval-2015-testset.txt', sep = '\\t')\ntest_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-13T13:08:41.886867Z","iopub.execute_input":"2023-01-13T13:08:41.887166Z","iopub.status.idle":"2023-01-13T13:08:41.926422Z","shell.execute_reply.started":"2023-01-13T13:08:41.887136Z","shell.execute_reply":"2023-01-13T13:08:41.925464Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"              tweetId                                          tweetText  \\\n0  578854927457349632  kereeen RT @Shyman33: Eclipse from ISS.... htt...   \n1  578874632670953472  Absolutely beautiful! RT @Shyman33: Eclipse fr...   \n2  578891261353984000  “@Shyman33: Eclipse from ISS.... http://t.co/C...   \n3  578846612312748032        Eclipse from ISS.... http://t.co/En87OtvsU6   \n4  578975333841551360  @ebonfigli: Éclipse vue de l'ISS... Autre chos...   \n\n       userId   imageId(s)         username                       timestamp  \\\n0    70824972  eclipse_01            peay_s  Fri Mar 20 09:45:43 +0000 2015   \n1   344707006  eclipse_01   JaredUcanChange  Fri Mar 20 11:04:02 +0000 2015   \n2   224839607  eclipse_01          tpjp1231  Fri Mar 20 12:10:06 +0000 2015   \n3   134543073  eclipse_01          Shyman33  Fri Mar 20 09:12:41 +0000 2015   \n4  1150728872   eclipse_01       Epimethee_  Fri Mar 20 17:44:11 +0000 2015   \n\n  label  \n0  fake  \n1  fake  \n2  fake  \n3  fake  \n4  fake  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweetId</th>\n      <th>tweetText</th>\n      <th>userId</th>\n      <th>imageId(s)</th>\n      <th>username</th>\n      <th>timestamp</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>578854927457349632</td>\n      <td>kereeen RT @Shyman33: Eclipse from ISS.... htt...</td>\n      <td>70824972</td>\n      <td>eclipse_01</td>\n      <td>peay_s</td>\n      <td>Fri Mar 20 09:45:43 +0000 2015</td>\n      <td>fake</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>578874632670953472</td>\n      <td>Absolutely beautiful! RT @Shyman33: Eclipse fr...</td>\n      <td>344707006</td>\n      <td>eclipse_01</td>\n      <td>JaredUcanChange</td>\n      <td>Fri Mar 20 11:04:02 +0000 2015</td>\n      <td>fake</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>578891261353984000</td>\n      <td>“@Shyman33: Eclipse from ISS.... http://t.co/C...</td>\n      <td>224839607</td>\n      <td>eclipse_01</td>\n      <td>tpjp1231</td>\n      <td>Fri Mar 20 12:10:06 +0000 2015</td>\n      <td>fake</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>578846612312748032</td>\n      <td>Eclipse from ISS.... http://t.co/En87OtvsU6</td>\n      <td>134543073</td>\n      <td>eclipse_01</td>\n      <td>Shyman33</td>\n      <td>Fri Mar 20 09:12:41 +0000 2015</td>\n      <td>fake</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>578975333841551360</td>\n      <td>@ebonfigli: Éclipse vue de l'ISS... Autre chos...</td>\n      <td>1150728872</td>\n      <td>eclipse_01</td>\n      <td>Epimethee_</td>\n      <td>Fri Mar 20 17:44:11 +0000 2015</td>\n      <td>fake</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_data['newFeature'] = test_data['tweetText'].apply(get_number_of_words)\ntest_data['tweetText'] = test_data['tweetText'].apply(preprocess_text)\ntest_data['tweetText'] = test_data['tweetText'].apply(stopword_removal)\ntest_data=test_data.dropna()\ntest_data['tweetTextTokens'] = test_data['tweetText'].apply(process_text)\ntest_data=test_data.dropna()\ntest_data['label'] = test_data['label'].apply(labels_to_numbers)\ntest_target = test_data['label']\ntest_data = test_data.drop(columns=['timestamp', 'username', 'userId', 'tweetText','label'])","metadata":{"execution":{"iopub.status.busy":"2023-01-13T13:08:45.588664Z","iopub.execute_input":"2023-01-13T13:08:45.589401Z","iopub.status.idle":"2023-01-13T13:09:13.798941Z","shell.execute_reply.started":"2023-01-13T13:08:45.589345Z","shell.execute_reply":"2023-01-13T13:09:13.797901Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"Evaluation","metadata":{}},{"cell_type":"code","source":"# train_data = data['tweetTextTokens']\ntrain_target = data['label']\ntrain_data = data.drop(columns=['label'])","metadata":{"execution":{"iopub.status.busy":"2023-01-13T13:10:53.573033Z","iopub.execute_input":"2023-01-13T13:10:53.573349Z","iopub.status.idle":"2023-01-13T13:10:53.581901Z","shell.execute_reply.started":"2023-01-13T13:10:53.573314Z","shell.execute_reply":"2023-01-13T13:10:53.580751Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"Vectorizing train and test set","metadata":{}},{"cell_type":"code","source":"tfid_vectorizer = TfidfVectorizer()\nvectorized_train = tfid_vectorizer.fit_transform(train_data['tweetTextTokens'])\nvectorized_test = tfid_vectorizer.transform(test_data['tweetTextTokens'])","metadata":{"execution":{"iopub.status.busy":"2023-01-13T13:10:55.755038Z","iopub.execute_input":"2023-01-13T13:10:55.755332Z","iopub.status.idle":"2023-01-13T13:10:55.968088Z","shell.execute_reply.started":"2023-01-13T13:10:55.755300Z","shell.execute_reply":"2023-01-13T13:10:55.967106Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"count_vectorizer = CountVectorizer()\nvectorized_train2 = count_vectorizer.fit_transform(train_data['tweetTextTokens'])\nvectorized_test2 = count_vectorizer.transform(test_data['tweetTextTokens'])","metadata":{"execution":{"iopub.status.busy":"2023-01-13T13:10:58.310816Z","iopub.execute_input":"2023-01-13T13:10:58.311142Z","iopub.status.idle":"2023-01-13T13:10:58.525091Z","shell.execute_reply.started":"2023-01-13T13:10:58.311068Z","shell.execute_reply":"2023-01-13T13:10:58.524099Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"Decision tree classifier:","metadata":{}},{"cell_type":"code","source":"dtree_clf2 = DecisionTreeClassifier(random_state=42)\ndtree_clf2.fit(vectorized_train2, train_target)\ndtree_pred2 = dtree_clf2.predict(vectorized_test2)\nprint (f1_score(test_target, dtree_pred2, average='micro'))\nprint(classification_report(test_target, dtree_pred2))","metadata":{"execution":{"iopub.status.busy":"2023-01-13T13:11:01.852840Z","iopub.execute_input":"2023-01-13T13:11:01.853134Z","iopub.status.idle":"2023-01-13T13:11:03.882678Z","shell.execute_reply.started":"2023-01-13T13:11:01.853092Z","shell.execute_reply":"2023-01-13T13:11:03.881735Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"0.8324900133155793\n              precision    recall  f1-score   support\n\n           0       0.68      0.91      0.78      1209\n           1       0.95      0.80      0.87      2546\n\n    accuracy                           0.83      3755\n   macro avg       0.81      0.85      0.82      3755\nweighted avg       0.86      0.83      0.84      3755\n\n","output_type":"stream"}]},{"cell_type":"code","source":"dtree_clf2","metadata":{"execution":{"iopub.status.busy":"2023-01-13T14:03:18.492136Z","iopub.execute_input":"2023-01-13T14:03:18.492411Z","iopub.status.idle":"2023-01-13T14:03:18.500942Z","shell.execute_reply.started":"2023-01-13T14:03:18.492381Z","shell.execute_reply":"2023-01-13T14:03:18.499869Z"},"trusted":true},"execution_count":62,"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"DecisionTreeClassifier(random_state=42)"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlog_clf = LogisticRegression(random_state=42,solver='liblinear',C= 100)\nlog_clf.fit(vectorized_train2, train_target)\nlog_pred = log_clf.predict(vectorized_test2)\nprint (f1_score(test_target, log_pred, average='micro'))\nprint(classification_report(test_target, log_pred))\n","metadata":{"execution":{"iopub.status.busy":"2023-01-13T13:11:07.148413Z","iopub.execute_input":"2023-01-13T13:11:07.148935Z","iopub.status.idle":"2023-01-13T13:11:07.658734Z","shell.execute_reply.started":"2023-01-13T13:11:07.148870Z","shell.execute_reply":"2023-01-13T13:11:07.657573Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"0.8090545938748336\n              precision    recall  f1-score   support\n\n           0       0.67      0.80      0.73      1209\n           1       0.90      0.81      0.85      2546\n\n    accuracy                           0.81      3755\n   macro avg       0.78      0.81      0.79      3755\nweighted avg       0.82      0.81      0.81      3755\n\n","output_type":"stream"}]},{"cell_type":"code","source":"nb_clf = MultinomialNB()\nnb_clf.fit(vectorized_train, train_target)\nnb_pred = nb_clf.predict(vectorized_test)\nprint (f1_score(test_target, nb_pred, average='micro'))\nprint(classification_report(test_target, nb_pred))","metadata":{"execution":{"iopub.status.busy":"2023-01-13T13:11:11.050689Z","iopub.execute_input":"2023-01-13T13:11:11.051782Z","iopub.status.idle":"2023-01-13T13:11:11.073897Z","shell.execute_reply.started":"2023-01-13T13:11:11.051736Z","shell.execute_reply":"2023-01-13T13:11:11.072776Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"0.6239680426098535\n              precision    recall  f1-score   support\n\n           0       0.45      0.75      0.56      1209\n           1       0.83      0.56      0.67      2546\n\n    accuracy                           0.62      3755\n   macro avg       0.64      0.66      0.62      3755\nweighted avg       0.70      0.62      0.64      3755\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#Import svm model\nfrom sklearn import svm\n\nclf = svm.SVC(kernel='rbf',C=100) # Linear Kernel\n\nclf.fit(vectorized_train, train_target)\n\ny_pred = clf.predict(vectorized_test)\n\nprint (f1_score(test_target, y_pred, average='micro'))\nprint(classification_report(test_target,y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2023-01-13T13:15:00.990183Z","iopub.execute_input":"2023-01-13T13:15:00.990821Z","iopub.status.idle":"2023-01-13T13:15:28.481666Z","shell.execute_reply.started":"2023-01-13T13:15:00.990784Z","shell.execute_reply":"2023-01-13T13:15:28.480742Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"0.8018641810918775\n              precision    recall  f1-score   support\n\n           0       0.68      0.74      0.71      1209\n           1       0.87      0.83      0.85      2546\n\n    accuracy                           0.80      3755\n   macro avg       0.77      0.79      0.78      3755\nweighted avg       0.81      0.80      0.80      3755\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#Import svm model\nfrom sklearn import svm\n\nclf = svm.SVC(kernel='rbf',C=100) # Linear Kernel\n\nclf.fit(vectorized_train, train_target)\n\ny_pred = clf.predict(vectorized_test)\n\nprint (f1_score(test_target, y_pred, average='micro'))\nprint(classification_report(test_target,y_pred))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Random Forrest Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nforest_clf = RandomForestClassifier(random_state=42)\nforest_clf.fit(vectorized_train, train_target)\nforest_pred = forest_clf.predict(vectorized_test)\nprint (f1_score(test_target, forest_pred, average='micro'))\nprint(classification_report(test_target, forest_pred))","metadata":{"execution":{"iopub.status.busy":"2023-01-13T14:14:20.927612Z","iopub.execute_input":"2023-01-13T14:14:20.927945Z","iopub.status.idle":"2023-01-13T14:14:34.329150Z","shell.execute_reply.started":"2023-01-13T14:14:20.927911Z","shell.execute_reply":"2023-01-13T14:14:34.328510Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"0.43315126489350836\n              precision    recall  f1-score   support\n\n           0       0.35      0.85      0.49      1209\n           1       0.77      0.25      0.37      2546\n\n    accuracy                           0.44      3755\n   macro avg       0.56      0.55      0.43      3755\nweighted avg       0.64      0.44      0.41      3755\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier()\nclassifier.fit(vectorized_train, train_target)\n\ny_pred_final = classifier.predict(vectorized_test)\nprint (f1_score(test_target, y_pred_final, average='micro'))\nprint(classification_report(test_target, y_pred_final ))\n","metadata":{"execution":{"iopub.status.busy":"2023-01-13T13:12:25.452212Z","iopub.execute_input":"2023-01-13T13:12:25.452551Z","iopub.status.idle":"2023-01-13T13:12:26.845188Z","shell.execute_reply.started":"2023-01-13T13:12:25.452501Z","shell.execute_reply":"2023-01-13T13:12:26.844307Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"0.7723035952063914\n              precision    recall  f1-score   support\n\n           0       0.61      0.83      0.70      1209\n           1       0.90      0.75      0.82      2546\n\n    accuracy                           0.77      3755\n   macro avg       0.75      0.79      0.76      3755\nweighted avg       0.81      0.77      0.78      3755\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# We'll use 100 weak learners to build a strong learner\nfrom sklearn.ensemble import AdaBoostClassifier\nclassifier = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=100)\nclassifier.fit(vectorized_train2, train_target)\ny_pred = classifier.predict(vectorized_test2)\n\nprint (f1_score(test_target, y_pred, average='micro'))\nprint(classification_report(test_target, y_pred ))","metadata":{"execution":{"iopub.status.busy":"2023-01-13T14:06:22.107582Z","iopub.execute_input":"2023-01-13T14:06:22.107887Z","iopub.status.idle":"2023-01-13T14:06:31.387106Z","shell.execute_reply.started":"2023-01-13T14:06:22.107855Z","shell.execute_reply":"2023-01-13T14:06:31.386264Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"0.836750998668442\n              precision    recall  f1-score   support\n\n           0       0.68      0.92      0.78      1209\n           1       0.96      0.80      0.87      2546\n\n    accuracy                           0.84      3755\n   macro avg       0.82      0.86      0.83      3755\nweighted avg       0.87      0.84      0.84      3755\n\n","output_type":"stream"}]}]}